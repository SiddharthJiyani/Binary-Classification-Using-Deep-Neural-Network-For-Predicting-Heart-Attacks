{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Binary Classification Using Deep Neural Network For Predicting Heart Attacks**\n",
        "\n",
        "\n",
        ">Submitted to: **Dr. Preety Singh**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wxwLjdNnT6g9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Import Libraries**\n"
      ],
      "metadata": {
        "id": "F_b7Z4-IUJEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "uvznITgoT9sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Loading and Preprocessing the Dataset**"
      ],
      "metadata": {
        "id": "ci2njT85UMx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    # Drop rows with missing values\n",
        "    data = data.dropna()\n",
        "\n",
        "    # Convert 'Result' to binary\n",
        "    data['Result'] = data['Result'].map({'negative': 0, 'positive': 1})\n",
        "\n",
        "    # Standardization to Normal Distribution with mean 0 and variance 1\n",
        "    scaler = StandardScaler()\n",
        "    features = ['Age', 'Gender', 'Heart rate', 'Systolic blood pressure',\n",
        "                'Diastolic blood pressure', 'Blood sugar', 'CK-MB', 'Troponin']\n",
        "\n",
        "    data[features] = scaler.fit_transform(data[features])\n",
        "\n",
        "    # Add polynomial features for important variables\n",
        "    data['CK_MB_squared'] = data['CK-MB'] ** 2\n",
        "    data['Troponin_squared'] = data['Troponin'] ** 2\n",
        "    data['Age_squared'] = data['Age'] ** 2\n",
        "\n",
        "    # Add interaction terms\n",
        "    data['CK_MB_Troponin'] = data['CK-MB'] * data['Troponin']\n",
        "    data['Age_BloodSugar'] = data['Age'] * data['Blood sugar']\n",
        "\n",
        "    features = features + ['CK_MB_squared', 'Troponin_squared', 'Age_squared',\n",
        "                          'CK_MB_Troponin', 'Age_BloodSugar']\n",
        "\n",
        "    X = data[features].values.T\n",
        "    Y = data['Result'].values.reshape(1, -1)\n",
        "\n",
        "    return data, X, Y"
      ],
      "metadata": {
        "id": "PQ8OwEevUOnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3: Neural Network Initialization**\n"
      ],
      "metadata": {
        "id": "EnkDsyS6UUqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layer_dims):\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)\n",
        "\n",
        "    for l in range(1, L):\n",
        "        parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2./layer_dims[l-1]) #He Normal Initialization because of ReLU activation used in hidden layers.\n",
        "        parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
        "\n",
        "    return parameters\n"
      ],
      "metadata": {
        "id": "Hp3rvBIaUVJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4: Forward Propagation**\n",
        "\n"
      ],
      "metadata": {
        "id": "kVPjdFT0Uc5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, parameters, keep_prob=0.8):\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    for l in range(1, L):\n",
        "        A_prev = A\n",
        "        W = parameters[f'W{l}']\n",
        "        b = parameters[f'b{l}']\n",
        "\n",
        "        Z = np.dot(W, A_prev) + b\n",
        "        A = np.maximum(0, Z)  # ReLU\n",
        "\n",
        "        # Apply dropout\n",
        "        D = np.random.rand(A.shape[0], A.shape[1]) < keep_prob\n",
        "        A = (A * D) / keep_prob #dividing by keep_prob to ensure that the expected output activations remain the same. Since number of neurons get reduced, we need to scale up the output accordingly.\n",
        "\n",
        "        cache = ((A_prev, W, b), Z, D)\n",
        "        caches.append(cache)\n",
        "\n",
        "    # Output layer with sigmoid\n",
        "    WL = parameters[f'W{L}']\n",
        "    bL = parameters[f'b{L}']\n",
        "    ZL = np.dot(WL, A) + bL\n",
        "    AL = 1 / (1 + np.exp(-ZL))\n",
        "\n",
        "    caches.append(((A, WL, bL), ZL))\n",
        "\n",
        "    return AL, caches"
      ],
      "metadata": {
        "id": "P7ved48vUcI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5: Cost Computation**\n"
      ],
      "metadata": {
        "id": "dFRn97ZvUb52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y, parameters, lambd=0.01):\n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Cross-entropy loss\n",
        "    cross_entropy_cost = -1/m * np.sum(Y * np.log(AL + 1e-8) + (1-Y) * np.log(1-AL + 1e-8)) #AL is y hat. Added 1e-8 to avoid computing log of 0.\n",
        "\n",
        "    # L2 regularization\n",
        "    L2_regularization_cost = 0\n",
        "    L = len(parameters) // 2 #Because the parameters data structure contains both weights and biases for each layer and we need only weights, therefore we divide by 2.\n",
        "    for l in range(L):\n",
        "        L2_regularization_cost += np.sum(np.square(parameters[f'W{l+1}']))\n",
        "    L2_regularization_cost *= lambd/(2*m)\n",
        "\n",
        "    return cross_entropy_cost + L2_regularization_cost"
      ],
      "metadata": {
        "id": "pAwpEpbHUh8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **6: Backward Propagation**\n"
      ],
      "metadata": {
        "id": "FvFwle6MUj9e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(AL, Y, caches, keep_prob=0.8, lambd=0.01):\n",
        "    grads = {}\n",
        "    L = len(caches)\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape)\n",
        "\n",
        "    # derivative of binary cross-entropy:\n",
        "    dAL = -(np.divide(Y, AL + 1e-8) - np.divide(1-Y, 1-AL + 1e-8))\n",
        "    current_cache = caches[L-1]\n",
        "\n",
        "    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(\n",
        "        dAL, current_cache, activation=\"sigmoid\", lambd=lambd)\n",
        "    grads[f\"dW{L}\"] = dW_temp\n",
        "    grads[f\"db{L}\"] = db_temp\n",
        "\n",
        "    # Hidden layers\n",
        "    for l in reversed(range(L-1)): #L-1 because the input layer doesn't count in hidden layers\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(\n",
        "            dA_prev_temp, current_cache, activation=\"relu\", lambd=lambd, keep_prob=keep_prob)\n",
        "        grads[f\"dW{l+1}\"] = dW_temp\n",
        "        grads[f\"db{l+1}\"] = db_temp\n",
        "\n",
        "    return grads\n",
        "\n",
        "#Funciton to perform backward propagation for single layer in the neural network:\n",
        "def linear_activation_backward(dA, cache, activation, lambd=0.01, keep_prob=0.8):\n",
        "    linear_cache, activation_cache, D = cache if len(cache) == 3 else (*cache, None)\n",
        "    A_prev, W, b = linear_cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        dZ = dA * (activation_cache > 0)\n",
        "        if D is not None:  # Apply dropout\n",
        "            dZ = dZ * D / keep_prob\n",
        "    elif activation == \"sigmoid\":\n",
        "        s = 1/(1+np.exp(-activation_cache))\n",
        "        dZ = dA * s * (1-s)\n",
        "\n",
        "    dW = 1./m * np.dot(dZ, A_prev.T) + (lambd/m) * W  #Derivative changed due to L2 Regularization\n",
        "    db = 1./m * np.sum(dZ, axis=1, keepdims=True)\n",
        "    dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "    return dA_prev, dW, db"
      ],
      "metadata": {
        "id": "5qLrG3AUUljm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **7: Update Parameters**\n"
      ],
      "metadata": {
        "id": "dx9U1L2mUqIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[f\"W{l+1}\"] -= learning_rate * grads[f\"dW{l+1}\"]\n",
        "        parameters[f\"b{l+1}\"] -= learning_rate * grads[f\"db{l+1}\"]\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "7QViuKnnUpA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **8: Model Definition**\n"
      ],
      "metadata": {
        "id": "IhL24_GHU0_0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model(X, Y, layers_dims, learning_rate=0.01, num_epochs=3000, batch_size=32,\n",
        "          keep_prob=0.8, lambd=0.01, print_cost=True):\n",
        "    costs = []\n",
        "\n",
        "    # Initialize parameters\n",
        "    parameters = initialize_parameters(layers_dims)\n",
        "\n",
        "    # Implement mini-batch gradient descent\n",
        "    num_complete_batches = X.shape[1] // batch_size\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        epoch_cost = 0\n",
        "\n",
        "        # Shuffle the data\n",
        "        permutation = np.random.permutation(X.shape[1])\n",
        "        X_shuffled = X[:, permutation]\n",
        "        Y_shuffled = Y[:, permutation]\n",
        "\n",
        "        for k in range(num_complete_batches):\n",
        "            # Get mini-batch\n",
        "            start_idx = k * batch_size\n",
        "            end_idx = (k + 1) * batch_size\n",
        "            X_batch = X_shuffled[:, start_idx:end_idx]\n",
        "            Y_batch = Y_shuffled[:, start_idx:end_idx]\n",
        "\n",
        "            AL, caches = forward_propagation(X_batch, parameters, keep_prob)\n",
        "            cost = compute_cost(AL, Y_batch, parameters, lambd)\n",
        "            grads = backward_propagation(AL, Y_batch, caches, keep_prob, lambd)\n",
        "            parameters = update_parameters(parameters, grads, learning_rate)\n",
        "\n",
        "            epoch_cost += cost\n",
        "\n",
        "        # Print the cost every 100 epochs\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print(f\"Cost after epoch {i}: {epoch_cost}\")\n",
        "        if print_cost :\n",
        "            costs.append(epoch_cost)\n",
        "\n",
        "    return parameters, costs"
      ],
      "metadata": {
        "id": "EdKXQNsuUxUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **9: Load and Preprocess Data**\n",
        "\n"
      ],
      "metadata": {
        "id": "FN3JFLybU4O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('Medicaldataset.csv')\n",
        "data, X, Y = preprocess_data(data)"
      ],
      "metadata": {
        "id": "v6tKLUoLU6n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **10: Plot Functions**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ki2-dQlAU-uk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_scatterplots(df):\n",
        "    # List of columns to plot\n",
        "    numerical_columns = ['Age', 'Heart rate', 'Systolic blood pressure', 'Diastolic blood pressure', 'Blood sugar', 'CK-MB', 'Troponin']\n",
        "\n",
        "    # Color mapping for the 'Result' column: positive = red, negative = green\n",
        "    color_map = {'positive': 'red', 'negative': 'green'}\n",
        "    colors = df['Result'].map(color_map)\n",
        "\n",
        "    # Create scatter plots for each pair of columns\n",
        "    num_cols = len(numerical_columns)\n",
        "    plt.figure(figsize=(16, 12))\n",
        "\n",
        "    plot_number = 1\n",
        "    for i in range(num_cols):\n",
        "        for j in range(i + 1, num_cols):\n",
        "            plt.subplot(num_cols - 1, num_cols - 1, plot_number)\n",
        "            plt.scatter(df[numerical_columns[i]], df[numerical_columns[j]], c=colors)\n",
        "            plt.xlabel(numerical_columns[i])\n",
        "            plt.ylabel(numerical_columns[j])\n",
        "            plot_number += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_correlation_matrix(df):\n",
        "    # List of numerical columns\n",
        "    numerical_columns = ['Age', 'Heart rate', 'Systolic blood pressure', 'Diastolic blood pressure', 'Blood sugar', 'CK-MB', 'Troponin']\n",
        "    corr_matrix = df[numerical_columns].corr()\n",
        "    # Use only upper triangle\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Create a heatmap using seaborn\n",
        "    sns.heatmap(corr_matrix, annot=True, mask=mask, cmap=\"coolwarm\", vmin=-1, vmax=1, square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
        "    plt.title(\"Correlation Matrix of Medical Data\", fontsize=16)\n",
        "    plt.show()\n",
        "\n",
        "# Generate the Plots\n",
        "untouchedData = pd.read_csv('Medicaldataset.csv')\n",
        "plot_scatterplots(untouchedData)\n",
        "print(' ')\n",
        "plot_correlation_matrix(untouchedData)"
      ],
      "metadata": {
        "id": "gofEXb7jU8eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **11: Make Predictions and Evaluate**\n"
      ],
      "metadata": {
        "id": "JlSOfrhiVTeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X.T, Y.T, test_size=0.2, random_state=42)\n",
        "X_train, X_test = X_train.T, X_test.T\n",
        "Y_train, Y_test = Y_train.T, Y_test.T\n",
        "\n",
        "# Define layer dimensions\n",
        "layers_dims = [X_train.shape[0], 15, 8, 1]  # 2 hidden layers\n",
        "\n",
        "# Train the model\n",
        "parameters, costs = model(X_train, Y_train, layers_dims,\n",
        "                         learning_rate=0.01,\n",
        "                         num_epochs=3000,\n",
        "                         batch_size=32,\n",
        "                         keep_prob=0.8,\n",
        "                         lambd=0.01)\n",
        "\n",
        "def plot_costs(costs):\n",
        "    epochs = range(len(costs))\n",
        "\n",
        "    # Plotting the cost vs. epochs\n",
        "    print('  ')\n",
        "    plt.plot(epochs, costs, label='Cost')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.title('Cost vs. Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "plot_costs(costs)\n",
        "\n",
        "\n",
        "# Generate predictions for the test set\n",
        "Y_test_predicted, _ = forward_propagation(X_test, parameters, keep_prob=1.0)  # No dropout during testing\n",
        "Y_test_predicted = (Y_test_predicted > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(Y_test.flatten(), Y_test_predicted.flatten())\n",
        "\n",
        "# Display the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Negative', 'Positive'])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "# Calculate accuracy\n",
        "def calculate_accuracy(X, Y, parameters):\n",
        "    AL, _ = forward_propagation(X, parameters, keep_prob=1.0)  # No dropout during testing\n",
        "    predictions = (AL > 0.5).astype(int)\n",
        "    accuracy = np.mean(predictions == Y) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Print final accuracies\n",
        "train_accuracy = calculate_accuracy(X_train, Y_train, parameters)\n",
        "test_accuracy = calculate_accuracy(X_test, Y_test, parameters)\n",
        "print(f\"Final Training Accuracy: {train_accuracy:.2f}%\")\n",
        "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "PUX2QaHlVbxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jcd8bm6QXyov"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}